local bert_model = "bert-base-uncased";
local cwe_info_file = "data1/CWE_info.json";
local seed = 2023;

{
    "random_seed": seed,
    "numpy_seed": seed,
    "pytorch_seed": seed,
    "dataset_reader": {
        "type": "reader_tree",
        "sample_neg": 0.01,
        "tokenizer": {
            "type": "pretrained_transformer",
            "model_name": bert_model,
            "add_special_tokens": true,
            "max_length": 256
        },
        "token_indexers": {
            "tokens": {
                "type": "pretrained_transformer",
                "model_name": bert_model,
                "namespace": "tags"
            }
        }
    },
    "train_data_path": "data1/train_samples_compressed.csv",
    "validation_data_path": "data1/validation_samples_compressed.csv",
    "model": {
        "type": "model_tree",
        "dropout": 0.1,
        "PTM": bert_model,
        "cwe_info_file": cwe_info_file,
        "level_num": 3,
        "upper_level_result_awareness": false,
        "loss_weight": [
            1,
            0.25,
            0.25
        ],
        // TODO: change thres for test
        "root_thres": 0.5,
        "update_step": 64,
        "text_field_embedder": {
            "token_embedders": {
                "tokens": {
                    "type": "custom_pretrained_transformer",
                    "model_name": bert_model,
                    "train_parameters": true,
                    "pretrained_model_path": "further_pretrain/out_wwm/"
                }
            }
        }
    },
    "data_loader": {
        "batch_size": 16,
        "shuffle": false,
        "drop_last": true,
        // "max_instances_in_memory": 8192
    },
    "validation_data_loader": {
        "batch_size": 512,
        "shuffle": false, // global shuffle in reader_tree
        // "max_instances_in_memory": 8192,
        // "batches_per_epoch": 128
    },
    "trainer": {
        "type": "gradient_descent",
        "optimizer": {
            "type": "huggingface_adamw",
            "parameter_groups": [
                [
                    [
                        "_text_field_embedder"
                    ],
                    {
                        "lr": 2e-5,
                        "requires_grad": true
                    }
                ],
                [
                    [
                        "_root_pooler"
                    ],
                    {
                        "lr": 5e-5,
                        "requires_grad": true
                    }
                ]
            ],
            "lr": 1e-4,
            "betas": [
                0.9,
                0.999
            ]
        },
        "checkpointer": {
            "keep_most_recent_by_count": 1,
        },
        "callbacks": [
            {
                "type": "reload_training_data",
                "change_neg_ratio_after": {
                    "5": 0.02,
                    "10": 0.04,
                }
            },
            {
                "type": "dynamic_loss_weight",
                "epoch_weight_map": {
                    "5": [
                        0.4,
                        0.3,
                        0.3
                    ],
                    "10": [
                        0.333,
                        0.333,
                        0.333
                    ]
                }
            },
            {
                "type": "change_teacher_forcing_ratio",
                "decay_strategy": "inverse_sigmoid",
                "k": 13,
                "start_learning_ratio": 1.0,
                "start_learning_epochs": 5,
                "end_learning_ratio": 0,
                "end_learning_epochs": 40,
            }
        ],
        "learning_rate_scheduler": {
            "type": "linear_with_warmup",
            "warmup_steps": 5000
        },
        // "num_gradient_accumulation_steps": 2,
        "validation_metric": "+f1", // maximize f1 score of root or use default "-loss"
        "num_epochs": 50,
        "patience": 10,
        // TODO: this is the real place to change target gpu
        "cuda_device": 1
    },
}